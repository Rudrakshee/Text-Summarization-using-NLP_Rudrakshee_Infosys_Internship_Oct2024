{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPv2a9R1kLpPG6MYhzpcYJK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rRul_nrQU3k","executionInfo":{"status":"ok","timestamp":1731695262788,"user_tz":-330,"elapsed":25234,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}},"outputId":"b9a99c88-b2cf-4472-871c-2e2841b23872"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Directory /content/drive/My Drive/AA exists!\n","Files in the directory: ['wiki_09.txt', 'wiki_07.txt', 'wiki_00.txt', 'wiki_01.txt', 'wiki_03.txt', 'wiki_12.txt', 'wiki_10.txt', 'wiki_04.txt', 'wiki_05.txt', 'wiki_11.txt', 'wiki_02.txt', 'wiki_13.txt', 'wiki_06.txt', 'wiki_08.txt', 'wiki_17.txt', 'wiki_34.txt', 'wiki_16.txt', 'wiki_39.txt', 'wiki_18.txt', 'wiki_47.txt', 'wiki_37.txt', 'wiki_31.txt', 'wiki_24.txt', 'wiki_41.txt', 'wiki_30.txt', 'wiki_45.txt', 'wiki_52.txt', 'wiki_51.txt', 'wiki_28.txt', 'wiki_50.txt', 'wiki_49.txt', 'wiki_35.txt', 'wiki_40.txt', 'wiki_55.txt', 'wiki_36.txt', 'wiki_19.txt', 'wiki_33.txt', 'wiki_15.txt', 'wiki_21.txt', 'wiki_42.txt', 'wiki_25.txt', 'wiki_46.txt', 'wiki_23.txt', 'wiki_29.txt', 'wiki_27.txt', 'wiki_54.txt', 'wiki_44.txt', 'wiki_48.txt', 'wiki_26.txt', 'wiki_32.txt', 'wiki_56.txt', 'wiki_43.txt', 'wiki_14.txt', 'wiki_22.txt', 'wiki_53.txt', 'wiki_38.txt', 'wiki_20.txt', 'wiki_86.txt', 'wiki_97.txt', 'wiki_90.txt', 'wiki_79.txt', 'wiki_85.txt', 'wiki_67.txt', 'wiki_74.txt', 'wiki_60.txt', 'wiki_80.txt', 'wiki_71.txt', 'wiki_87.txt', 'wiki_68.txt', 'wiki_84.txt', 'wiki_93.txt', 'wiki_63.txt', 'wiki_75.txt', 'wiki_94.txt', 'wiki_88.txt', 'wiki_98.txt', 'wiki_91.txt', 'wiki_57.txt', 'wiki_77.txt', 'wiki_72.txt', 'wiki_69.txt', 'wiki_76.txt', 'wiki_83.txt', 'wiki_92.txt', 'wiki_59.txt', 'wiki_70.txt', 'wiki_78.txt', 'wiki_73.txt', 'wiki_65.txt', 'wiki_61.txt', 'wiki_62.txt', 'wiki_96.txt', 'wiki_89.txt', 'wiki_95.txt', 'wiki_58.txt', 'wiki_82.txt', 'wiki_66.txt', 'wiki_64.txt', 'wiki_81.txt', 'wiki_99.txt']\n"]}],"source":["import os\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Specify the folder path\n","folder_path = '/content/drive/My Drive/AA'\n","\n","# Debug: Check if the folder exists\n","if not os.path.exists(folder_path):\n","    print(f\"Directory {folder_path} does not exist!\")\n","else:\n","    print(f\"Directory {folder_path} exists!\")\n","\n","# List file names if folder exists\n","file_names = os.listdir(folder_path)\n","print(\"Files in the directory:\", file_names)\n"]},{"cell_type":"code","source":["# Import necessary libraries\n","!pip install langdetect\n","!pip install python-docx\n","# Required Libraries\n","import os\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from langdetect import detect\n","import docx  # Importing python-docx for saving to Word"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cNROZHhQqnH","executionInfo":{"status":"ok","timestamp":1731681539351,"user_tz":-330,"elapsed":22530,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}},"outputId":"0d8f126c-82bf-4a83-d9ec-623453a29f13"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/981.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=7522724a19088e2d03ab22afcb6c48c6ad4e5cedd5e247599f92c2c2aa938a93\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n"]}]},{"cell_type":"code","source":["# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8PSBD9fQ2OU","executionInfo":{"status":"ok","timestamp":1731681575190,"user_tz":-330,"elapsed":475,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}},"outputId":"5f39e9ef-125b-48fe-b58c-f86ed30d1d92"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Synonym Replacement Dictionary\n","synonym_dict = {\n","    \"NLP\": \"Natural Language Processing\",\n","    \"AI\": \"Artificial Intelligence\",\n","    \"ML\": \"Machine Learning\",\n","    \"stats\": \"statistics\",\n","    # Add more terms as needed\n","}"],"metadata":{"id":"zaUn6CMAQ-n3","executionInfo":{"status":"ok","timestamp":1731681619200,"user_tz":-330,"elapsed":424,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n","    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n","    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    return text"],"metadata":{"id":"r9d-toTARJ9X","executionInfo":{"status":"ok","timestamp":1731681653004,"user_tz":-330,"elapsed":415,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","def remove_stop_words(text):\n","    tokens = word_tokenize(text)\n","    filtered_words = [word for word in tokens if word not in stop_words]\n","    return ' '.join(filtered_words)"],"metadata":{"id":"Ij3amupJRSMi","executionInfo":{"status":"ok","timestamp":1731681675443,"user_tz":-330,"elapsed":431,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","def lemmatize_text(text):\n","    tokens = word_tokenize(text)\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n","    return ' '.join(lemmatized_words)"],"metadata":{"id":"Dq5SPpS8RYQK","executionInfo":{"status":"ok","timestamp":1731681705992,"user_tz":-330,"elapsed":426,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def is_english(text):\n","    try:\n","        return detect(text) == 'en'\n","    except:\n","        return False"],"metadata":{"id":"yQim3jwcRgLI","executionInfo":{"status":"ok","timestamp":1731681732795,"user_tz":-330,"elapsed":438,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def tokenize_text(text):\n","    sentences = nltk.sent_tokenize(text)\n","    words = word_tokenize(text)\n","    return sentences, words"],"metadata":{"id":"pNxHcsrhRlrm","executionInfo":{"status":"ok","timestamp":1731681755495,"user_tz":-330,"elapsed":420,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def pos_tagging(text):\n","    tokens = word_tokenize(text)\n","    pos_tags = nltk.pos_tag(tokens)\n","    return pos_tags"],"metadata":{"id":"5J9PpCG-R2kd","executionInfo":{"status":"ok","timestamp":1731681812436,"user_tz":-330,"elapsed":402,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def named_entity_recognition(text):\n","    tokens = word_tokenize(text)\n","    pos_tags = nltk.pos_tag(tokens)\n","    ner_tree = nltk.ne_chunk(pos_tags, binary=True)  # Binary=True identifies only named entities\n","    return ner_tree"],"metadata":{"id":"tqONdmn2R5LS","executionInfo":{"status":"ok","timestamp":1731681841815,"user_tz":-330,"elapsed":500,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def replace_synonyms(text):\n","    words = text.split()\n","    replaced_text = ' '.join([synonym_dict.get(word, word) for word in words])\n","    return replaced_text"],"metadata":{"id":"f4aHPR_CSAYU","executionInfo":{"status":"ok","timestamp":1731681861076,"user_tz":-330,"elapsed":1171,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def vectorize_tfidf(documents):\n","    tfidf = TfidfVectorizer()\n","    tfidf_matrix = tfidf.fit_transform(documents)\n","    return tfidf_matrix, tfidf.get_feature_names_out()"],"metadata":{"id":"141ShxjXSEdp","executionInfo":{"status":"ok","timestamp":1731681877193,"user_tz":-330,"elapsed":417,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def pad_sequences_data(documents, max_length=50):\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(documents)\n","    sequences = tokenizer.texts_to_sequences(documents)\n","    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n","    return padded_sequences"],"metadata":{"id":"6CEz1LHCSIG5","executionInfo":{"status":"ok","timestamp":1731681897240,"user_tz":-330,"elapsed":440,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","    text = clean_text(text)\n","    text = remove_stop_words(text)\n","    text = lemmatize_text(text)\n","    text = replace_synonyms(text)  # Apply synonym replacement\n","    return text"],"metadata":{"id":"SA6_sX_6SOvG","executionInfo":{"status":"ok","timestamp":1731681918882,"user_tz":-330,"elapsed":519,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Step 1: Import required libraries\n","from google.colab import drive\n","import os\n","from docx import Document\n","\n","# Step 2: Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Step 3: Define the directory paths\n","input_dir = '/content/drive/My Drive/AA'  # Replace with the actual path to your text files\n","output_dir = '/content/drive/My Drive/AA_preprocessed_docs'\n","\n","# Create output directory if it doesn't exist\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Step 4: Create a summary document to collect processed text summaries\n","summary_doc = Document()\n","summary_doc.add_heading('Preprocessed Data Summary', 0)\n","\n","# Step 5: Process each file in the input directory\n","for file_name in os.listdir(input_dir):\n","    if file_name.endswith('.txt'):  # Only process text files\n","        file_path = os.path.join(input_dir, file_name)\n","\n","        # Step 6: Open and read each file, then preprocess\n","        with open(file_path, 'r') as file:\n","            content = file.read()\n","\n","            # Example preprocessing step (you can add more as needed)\n","            processed_content = content.lower()  # Converts all text to lowercase\n","\n","        # Step 7: Save the processed content to a new file in the output directory\n","        processed_file_path = os.path.join(output_dir, f'processed_{file_name}')\n","        with open(processed_file_path, 'w') as processed_file:\n","            processed_file.write(processed_content)\n","\n","        # Step 8: Add a summary entry for each file in the summary document\n","        summary_doc.add_paragraph(f\"Processed file: {file_name}\")\n","        summary_doc.add_paragraph(processed_content[:500])  # Adding the first 500 characters as a sample\n","\n","# Step 9: Save the summary document to the output directory\n","summary_summary_path = os.path.join(output_dir, 'preprocessed_data_summary.docx')\n","summary_doc.save(summary_summary_path)\n","\n","print(\"All preprocessed documents saved successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXKsITvBSUtd","executionInfo":{"status":"ok","timestamp":1731705701021,"user_tz":-330,"elapsed":6652,"user":{"displayName":"A_64_Rudrakshee Banerjee","userId":"15394019179541587728"}},"outputId":"dece4f26-5c45-4187-85b0-d9a416c510ca"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","All preprocessed documents saved successfully!\n"]}]}]}